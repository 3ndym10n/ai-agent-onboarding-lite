name: CI/CD Pipeline

on:
  push:
    branches: [main, develop, "feature/*"]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      test_level:
        description: "Test level to run"
        required: false
        default: "full"
        type: choice
        options:
          - smoke
          - unit
          - integration
          - full

env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1

jobs:
  validate-environment:
    name: Environment Validation
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.setup.outputs.python-version }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        id: setup
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Validate development environment
        run: |
          python scripts/maintenance/validate_dev_env.py

      - name: Check protected paths
        run: |
          python scripts/maintenance/protected_paths.py

  quality-checks:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    needs: validate-environment

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]
          pip install bandit safety pre-commit

      - name: Run pre-commit hooks
        run: |
          pre-commit run --all-files --verbose

      - name: Run security checks
        run: |
          echo "Running Bandit security scan..."
          bandit -r ai_onboard/ -f txt --severity-level medium || echo "Bandit security issues found"
          echo "Running Safety vulnerability check..."
          safety check || echo "Safety vulnerability issues found"

      - name: Upload code quality artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: code-quality-results
          path: |
            htmlcov/
            .coverage
          retention-days: 7

  smoke-tests:
    name: Smoke Tests
    runs-on: ubuntu-latest
    needs: validate-environment
    if: ${{ github.event_name != 'workflow_dispatch' || github.event.inputs.test_level == 'smoke' || github.event.inputs.test_level == 'full' }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Run smoke tests
        run: |
          python -m pytest tests/smoke/ -v --tb=short --junitxml=smoke-results.xml

      - name: Upload smoke test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: smoke-test-results
          path: smoke-results.xml
          retention-days: 7

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: validate-environment
    if: ${{ github.event_name != 'workflow_dispatch' || github.event.inputs.test_level == 'unit' || github.event.inputs.test_level == 'full' }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Run unit tests with coverage
        run: |
          python -m pytest tests/unit/ --cov=ai_onboard --cov-report=xml --cov-report=html --cov-report=term-missing -v --tb=short --junitxml=unit-results.xml --durations=10

      - name: Upload unit test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results
          path: |
            unit-results.xml
            htmlcov/
            .coverage
          retention-days: 7

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: validate-environment
    if: ${{ github.event_name != 'workflow_dispatch' || github.event.inputs.test_level == 'integration' || github.event.inputs.test_level == 'full' }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Run integration tests
        run: |
          python -m pytest tests/integration/ --cov=ai_onboard --cov-report=xml --cov-report=html --cov-append --cov-report=term-missing -v --tb=short --junitxml=integration-results.xml --durations=10

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            integration-results.xml
            htmlcov/
            .coverage
          retention-days: 7

  test-reporting:
    name: Test Results & Coverage
    runs-on: ubuntu-latest
    needs: [smoke-tests, unit-tests, integration-tests]
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Download test artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts/

      - name: Prepare environment
        run: |
          python -m pip install --upgrade pip
          pip install coverage junitparser

      - name: Generate aggregated test summary
        run: |
          python - <<'PY'
import json
from pathlib import Path
from junitparser import JUnitXml

artifacts_dir = Path("artifacts")
report = {
    "suites": [],
    "summary": {"tests": 0, "failures": 0, "errors": 0, "skipped": 0},
}

for xml_file in artifacts_dir.rglob("*.xml"):
    try:
        junit = JUnitXml.fromfile(xml_file)
    except Exception:
        continue

    suite_total = junit.tests or 0
    suite_failures = junit.failures or 0
    suite_errors = junit.errors or 0
    suite_skipped = junit.skipped or 0

    report["suites"].append(
        {
            "name": str(xml_file.relative_to(artifacts_dir)),
            "tests": suite_total,
            "failures": suite_failures,
            "errors": suite_errors,
            "skipped": suite_skipped,
        }
    )

    report["summary"]["tests"] += suite_total
    report["summary"]["failures"] += suite_failures
    report["summary"]["errors"] += suite_errors
    report["summary"]["skipped"] += suite_skipped

report_path = Path("report.md")
with report_path.open("w", encoding="utf-8") as handle:
    handle.write("# Test Summary\\n\\n")
    summary = report["summary"]
    handle.write(
        f"- Total tests: {summary['tests']}\\n"
        f"- Failures: {summary['failures']}\\n"
        f"- Errors: {summary['errors']}\\n"
        f"- Skipped: {summary['skipped']}\\n\\n"
    )

    if report["suites"]:
        handle.write("## Suites\\n")
        for suite in sorted(report["suites"], key=lambda item: item["name"]):
            handle.write(
                f"- {suite['name']}: {suite['tests']} tests, "
                f"{suite['failures']} failures, {suite['errors']} errors, "
                f"{suite['skipped']} skipped\\n"
            )

report_json = Path("report.json")
report_json.write_text(json.dumps(report, indent=2))
PY

      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: |
            report.md
            report.json
          retention-days: 30

      - name: Coverage Report
        run: |
          unit_cov="artifacts/unit-test-results/.coverage"
          integ_cov="artifacts/integration-test-results/.coverage"
          found=0

          if [ -f "$unit_cov" ]; then
            cp "$unit_cov" .coverage.unit
            found=1
          fi

          if [ -f "$integ_cov" ]; then
            cp "$integ_cov" .coverage.integration
            found=1
          fi

          if [ "$found" -eq 1 ]; then
            coverage combine .coverage.unit .coverage.integration 2>/dev/null || true
            coverage report --show-missing
            coverage html
          else
            echo "No coverage files found; skipping combine."
          fi

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: dorny/test-reporter@v1
        with:
          name: Test Results
          path: "artifacts/**/*.xml"
          reporter: java-junit
          fail-on-error: false


name: Test Reporting Dashboard

on:
  workflow_run:
    workflows: ["CI/CD Pipeline"]
    types: [completed]
  schedule:
    # Run daily at 6 AM UTC
    - cron: "0 6 * * *"
  workflow_dispatch:

jobs:
  collect-test-data:
    name: Collect Test Metrics
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_run'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install junitparser

      - name: Download test artifacts from CI workflow
        uses: dawidd6/action-download-artifact@v2
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          workflow: ci.yml
          run_id: ${{ github.event.workflow_run.id }}
          path: test-artifacts

      - name: Summarize test results
        run: |
          python - <<'PY'
import json
from pathlib import Path
from junitparser import JUnitXml

artifacts = Path('test-artifacts')
summary_dir = Path('dashboard-data')
summary_dir.mkdir(parents=True, exist_ok=True)

summary = {
    'suites': [],
    'totals': {'tests': 0, 'failures': 0, 'errors': 0, 'skipped': 0},
}

for xml_file in artifacts.rglob('*.xml'):
    try:
        junit = JUnitXml.fromfile(xml_file)
    except Exception:
        continue

    data = {
        'file': str(xml_file.relative_to(artifacts)),
        'tests': junit.tests or 0,
        'failures': junit.failures or 0,
        'errors': junit.errors or 0,
        'skipped': junit.skipped or 0,
    }

    for key in summary['totals']:
        summary['totals'][key] += data[key]

    summary['suites'].append(data)

summary['files_processed'] = len(summary['suites'])
(summary_dir / 'summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')

with (summary_dir / 'summary.md').open('w', encoding='utf-8') as handle:
    handle.write('# Test Results Summary

')
    totals = summary['totals']
    handle.write(f"- Total tests: {totals['tests']}\n")
    handle.write(f"- Failures: {totals['failures']}\n")
    handle.write(f"- Errors: {totals['errors']}\n")
    handle.write(f"- Skipped: {totals['skipped']}\n\n")
    if summary['suites']:
        handle.write('## Suites\n')
        for suite in summary['suites']:
            handle.write(
                f"- {suite['file']}: {suite['tests']} tests, "
                f"{suite['failures']} failures, {suite['errors']} errors, "
                f"{suite['skipped']} skipped\n"
            )
PY

      - name: Upload dashboard data
        uses: actions/upload-artifact@v3
        with:
          name: test-dashboard
          path: |
            dashboard-data/summary.json
            dashboard-data/summary.md
          retention-days: 90

  quality-metrics:
    name: Code Quality Metrics
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install analysis tools
        run: |
          python -m pip install --upgrade pip
          pip install radon mccabe

      - name: Calculate code metrics
        run: |
          mkdir -p quality-metrics

          # Cyclomatic complexity
          python -c "
          import radon.complexity as cc
          import json
          from pathlib import Path

          results = {}
          for py_file in Path('ai_onboard').rglob('*.py'):
              if '__pycache__' not in str(py_file):
                  try:
                      with open(py_file, 'r', encoding='utf-8') as f:
                          content = f.read()
                      complexity = cc.average_complexity(cc.cc_visit(content))
                      results[str(py_file)] = complexity
                  except:
                      pass

          with open('quality-metrics/complexity.json', 'w') as f:
              json.dump(results, f, indent=2)
          "

          # Code metrics
          radon mi ai_onboard/ --json > quality-metrics/maintainability.json || true
          radon raw ai_onboard/ --json > quality-metrics/raw-metrics.json || true

      - name: Generate quality report
        run: |
          python - <<'PY'
import json
from pathlib import Path

metrics_dir = Path('quality-metrics')
report = Path('quality-report.md')
report_lines = ['# Quality Metrics Summary', '']

complexity = metrics_dir / 'complexity.json'
if complexity.exists():
    try:
        data = json.load(complexity.open())
        average = data.get('average')
        report_lines.append(f"Average complexity: {average}")
    except json.JSONDecodeError:
        report_lines.append('Unable to parse complexity data.')
else:
    report_lines.append('Complexity data not available.')

maintain = metrics_dir / 'maintainability.json'
if maintain.exists():
    try:
        data = json.load(maintain.open())
        average = data.get('average')
        report_lines.append(f"Maintainability index: {average}")
    except json.JSONDecodeError:
        report_lines.append('Unable to parse maintainability data.')
else:
    report_lines.append('Maintainability data not available.')

report_lines.append('')
report.write_text('\n'.join(report_lines), encoding='utf-8')
PY

      - name: Upload quality metrics
        uses: actions/upload-artifact@v3
        with:
          name: quality-metrics
          path: |
            quality-metrics/
            quality-report.md
          retention-days: 90

  publish-dashboard:
    name: Publish Dashboard
    runs-on: ubuntu-latest
    needs: [collect-test-data, quality-metrics]
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts

      - name: Combine dashboard data
        run: |
          mkdir -p public
          cp artifacts/test-dashboard/dashboard-data/summary.md public/test-summary.md 2>/dev/null || true
          cp artifacts/quality-metrics/quality-report.md public/quality-summary.md 2>/dev/null || true
          python - <<'PY'
from pathlib import Path
import datetime

public = Path('public')
public.mkdir(exist_ok=True)

sections = []
summary = Path('public/test-summary.md')
if summary.exists():
    sections.append(summary.read_text(encoding='utf-8'))
quality = Path('public/quality-summary.md')
if quality.exists():
    sections.append(quality.read_text(encoding='utf-8'))

content = ['# AI-Onboard Quality Dashboard', '']
if sections:
    content.extend(sections)
else:
    content.append('No dashboard data available.')

timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')
content.append('Last updated: ' + timestamp)
Path('public/index.md').write_text('

'.join(content), encoding='utf-8')
PY

      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          cname: quality.ai-onboard.com




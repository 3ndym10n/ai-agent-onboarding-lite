name: Performance Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      users:
        description: "Number of concurrent users"
        required: false
        default: "20"
        type: string
      duration:
        description: "Test duration in seconds"
        required: false
        default: "60"
        type: string
      benchmark_only:
        description: "Run only benchmark tests (skip load tests)"
        required: false
        default: false
        type: boolean

jobs:
  performance-regression:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test]

      - name: Run performance regression tests
        run: |
          python -m pytest tests/performance/test_performance_regression.py \
            --benchmark-json=benchmark_results.json \
            --benchmark-save=latest \
            --benchmark-compare \
            --benchmark-compare-fail=mean:10% \
            -v

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results.json

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      has-results: ${{ steps.capture.outputs.has_results }}

    services:
      # Start the performance testing API server
      api-server:
        image: python:3.11-slim
        ports:
          - 8080:8080
        options: >-
          --health-cmd "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8080/api/health\", timeout=5)'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        env:
          PYTHONPATH: /app

    steps:
      - name: Determine load testing execution
        id: gate
        run: |
          benchmark_only="${{ github.event.inputs.benchmark_only || 'false' }}"
          if [ "$benchmark_only" = "true" ]; then
            echo "run=false" >> $GITHUB_OUTPUT
          else
            echo "run=true" >> $GITHUB_OUTPUT
          fi

      - name: Checkout code
        if: steps.gate.outputs.run == 'true'
        uses: actions/checkout@v4

      - name: Set up Python
        if: steps.gate.outputs.run == 'true'
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        if: steps.gate.outputs.run == 'true'
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test]

      - name: Start performance API server
        if: steps.gate.outputs.run == 'true'
        run: |
          python tests/performance/performance_api_server.py --host 0.0.0.0 --port 8080 &
          python - <<'PY'
import sys
import time
import urllib.request

url = "http://localhost:8080/api/health"
for _ in range(30):
    try:
        urllib.request.urlopen(url, timeout=5)
        sys.exit(0)
    except Exception:
        time.sleep(1)
sys.exit('Service did not become ready in time')
PY

      - name: Run load tests
        if: steps.gate.outputs.run == 'true'
        env:
          USERS: ${{ github.event.inputs.users || '20' }}
          DURATION: ${{ github.event.inputs.duration || '60' }}
        run: |
          python scripts/performance/run_performance_tests.py             --users $USERS             --duration $DURATION             --host http://localhost:8080             --skip-regression-tests

      - name: Upload load test results
        if: steps.gate.outputs.run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: performance_results/

      - name: Record load test status
        id: capture
        run: |
          if [ "${{ steps.gate.outputs.run }}" = "true" ]; then
            echo "has_results=true" >> $GITHUB_OUTPUT
          else
            echo "has_results=false" >> $GITHUB_OUTPUT
            echo "Load testing skipped for this run." >> $GITHUB_STEP_SUMMARY
          fi

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: [performance-regression, load-testing]
    if: always() && (needs.performance-regression.result == 'success' || needs.load-testing.result == 'success')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results/

      - name: Download load test results
        if: needs.load-testing.outputs.has-results == 'true'
        uses: actions/download-artifact@v4
        with:
          name: load-test-results
          path: load_test_results/

      - name: Analyze performance results
        run: |
          echo "## Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "benchmark_results/benchmark_results.json" ]; then
            echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "- Performance regression tests completed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.load-testing.outputs.has-results }}" = "true" ] && [ -d "load_test_results" ] && [ "$(ls -A load_test_results)" ]; then
            echo "### Load Test Results" >> $GITHUB_STEP_SUMMARY
            echo "- Load testing completed" >> $GITHUB_STEP_SUMMARY
            find load_test_results -name "*.md" -exec head -20 {} \; 2>/dev/null || true
            echo "" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.load-testing.outputs.has-results }}" != "true" ]; then
            echo "### Load Test Results" >> $GITHUB_STEP_SUMMARY
            echo "- Load testing skipped for this run" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "### Performance Status" >> $GITHUB_STEP_SUMMARY
          if [ "${{ needs.performance-regression.result }}" = "success" ]; then
            echo "- Regression tests: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- Regression tests: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.load-testing.result }}" = "success" ] && [ "${{ needs.load-testing.outputs.has-results }}" = "true" ]; then
            echo "- Load tests: PASSED" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.load-testing.outputs.has-results }}" != "true" ]; then
            echo "- Load tests: SKIPPED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- Load tests: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## Performance Test Results\n\n';

            if (fs.existsSync('benchmark_results/benchmark_results.json')) {
              comment += '### Benchmark Tests\n';
              comment += 'Performance regression tests completed successfully.\n\n';
            }

            if ('${{ needs.load-testing.outputs.has-results }}' === 'true' && fs.existsSync('load_test_results')) {
              const files = fs.readdirSync('load_test_results');
              if (files.length > 0) {
                comment += '### Load Tests\n';
                comment += 'Load testing completed. Check artifacts for detailed results.\n\n';
              }
            } else if ('${{ needs.load-testing.outputs.has-results }}' !== 'true') {
              comment += '### Load Tests\n';
              comment += 'Load testing was skipped for this run.\n\n';
            }

            comment += '**[View detailed results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})**\n\n';
            comment += '*Performance tests ensure code changes do not introduce regressions.*';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });



  performance-alert:
    name: Performance Alert
    runs-on: ubuntu-latest
    needs: [performance-regression]
    if: failure() && github.event_name == 'pull_request'

    steps:
      - name: Performance regression detected
        run: |
          echo 'Performance regression detected!'
          echo 'The performance regression tests failed, indicating that'
          echo 'this PR may have introduced performance degradation.'
          echo ''
          echo 'Please review the benchmark results and consider:'
          echo '- Optimizing critical code paths'
          echo '- Reducing memory allocations'
          echo '- Minimizing I/O operations'
          echo '- Caching expensive computations'

      - name: Comment performance alert on PR
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '## Performance Regression Alert\n\n'
                + 'The performance regression tests have detected potential performance degradation in this PR.\n\n'
                + '**Action Required:**\n'
                + '- Review the benchmark results in the workflow artifacts\n'
                + '- Consider optimizing costly algorithms\n'
                + '- Check for unnecessary memory allocations or I/O operations\n'
                + '- Evaluate whether caching can help expensive operations\n\n'
                + '**[View Performance Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})**'
            });


